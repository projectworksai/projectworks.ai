# Token limits and alternative approaches

When the combined input (brief + tender + technical spec) approaches the model’s context limit, you can:

## 1. Stay within token limits (no new stack)

- **Truncation** (current): Long documents are capped at `MAX_DOCUMENT_CHARS` per document so the request fits.
- **Two-phase generation**:  
  1. First call: “Summarize this document in under 2000 words, keeping scope, requirements, and standards.”  
  2. Second call: Generate the full plan from the **brief + summaries** only. No need to send the full tender/spec again.
- **Chunking**: Split very long docs into chunks (e.g. by section or page), summarize each chunk in one call, then pass those summaries into the plan-generation call.
- **Larger-context model**: Use a model with a bigger context window via OpenRouter (e.g. Claude 100k) so you can send more of the original documents without summarization.

## 2. Custom AI in Python

Building “your own AI” in Python usually means:

- **Local LLM** (e.g. LLaMA, Mistral, Phi via `llama.cpp`, Ollama, or Hugging Face): runs on your machine or server; no per-request API cost; you control data. Context limits still apply (often 4k–32k tokens depending on model and hardware). Good for privacy and cost; you still need to truncate or summarize long inputs.
- **Small “summary” model**: A small local model (or a cheap API) that only does summarization; then your main plan is generated by the existing API using those summaries. This reduces tokens sent to the main model.
- **RAG (retrieval-augmented generation)**: Ingest tender/specs into a vector store; at plan time, retrieve only the most relevant chunks and send those to the model. Reduces tokens and can improve relevance.

Use Python when you want to run models locally, own the pipeline, or integrate RAG/chunking in a separate service. It does not remove token limits; it gives you more control over how you use them (summaries, chunks, retrieval).

## 3. What this app does today

- **Right column**: Shows **section summaries only** (main sections, no appendices). Full detail is in the **Word download**.
- **Single API call**: One generation call with truncated documents. For larger inputs, implement two-phase or chunking (see above) in the same API route or in a separate service.
